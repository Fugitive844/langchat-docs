<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>RAG 进阶分享 | LangChat Docs</title>
    <meta name="description" content="LangChat Project Document">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.wFIoVwSX.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.DPsL8uXk.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.CI_0JRYk.js">
    <link rel="modulepreload" href="/assets/chunks/framework.ByciF0Oj.js">
    <link rel="modulepreload" href="/assets/course_docs_20Rag.md.CFLP7nQ4.lean.js">
    <link rel="shortcut icon" href="/favicon.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-ed470ed6><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f444f753></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f444f753>Skip to content</a><!--]--><!----><header class="VPNav" data-v-ed470ed6 data-v-f09ebc71><div class="VPNavBar" data-v-f09ebc71 data-v-1044928a><div class="wrapper" data-v-1044928a><div class="container" data-v-1044928a><div class="title" data-v-1044928a><div class="VPNavBarTitle has-sidebar" data-v-1044928a data-v-c494e956><a class="title" href="/" data-v-c494e956><!--[--><!--]--><!----><span data-v-c494e956>LangChat Docs</span><!--[--><!--]--></a></div></div><div class="content" data-v-1044928a><div class="content-body" data-v-1044928a><!--[--><!--]--><div class="VPNavBarSearch search" data-v-1044928a><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-1044928a data-v-51f1f89a><span id="main-nav-aria-label" class="visually-hidden" data-v-51f1f89a> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/langchat/docs/introduce.html" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat文档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/course/docs/00Introduce.html" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>Java AI课程</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-51f1f89a data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-e2cb7df8><span class="text" data-v-e2cb7df8><!----><span data-v-e2cb7df8>在线预览</span><span class="vpi-chevron-down text-icon" data-v-e2cb7df8></span></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><div class="items" data-v-7750fdeb><!--[--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://llm.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat LLM Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://upms.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat UPMS Ops</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-1044928a data-v-adf2275d><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-adf2275d data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-1044928a data-v-17ef9bfb data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-1044928a data-v-a84198a3 data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-e2cb7df8><span class="vpi-more-horizontal icon" data-v-e2cb7df8></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><!----><!--[--><!--[--><!----><div class="group" data-v-a84198a3><div class="item appearance" data-v-a84198a3><p class="label" data-v-a84198a3>Appearance</p><div class="appearance-action" data-v-a84198a3><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-a84198a3 data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div></div></div><div class="group" data-v-a84198a3><div class="item social-links" data-v-a84198a3><div class="VPSocialLinks social-links-list" data-v-a84198a3 data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-1044928a data-v-5598f36f><span class="container" data-v-5598f36f><span class="top" data-v-5598f36f></span><span class="middle" data-v-5598f36f></span><span class="bottom" data-v-5598f36f></span></span></button></div></div></div></div><div class="divider" data-v-1044928a><div class="divider-line" data-v-1044928a></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-ed470ed6 data-v-d1ebcfd2><div class="container" data-v-d1ebcfd2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-d1ebcfd2><span class="vpi-align-left menu-icon" data-v-d1ebcfd2></span><span class="menu-text" data-v-d1ebcfd2>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-d1ebcfd2 data-v-8683af8e><button data-v-8683af8e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-ed470ed6 data-v-b5cecb30><div class="curtain" data-v-b5cecb30></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b5cecb30><span class="visually-hidden" id="sidebar-aria-label" data-v-b5cecb30> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChain4j基础</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/00Introduce.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>开始之前</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/01Models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型选择</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/02LangChain4j.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LangChain4j 介绍</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/03ChatAPI.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Chat API 上手</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/04ChatSetting.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>API 进阶配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/05ChatStream.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Chat 流式输出</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/06ChatVL.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Chat 视觉理解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/07ChatMemory.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Chat 记忆缓存</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/08Prompt.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>提示词工程</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/09JSONOutput.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>JSON 结构化输出</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/10DynamicJSON.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>业务动态 JSON 结构化</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/12Embedding.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>向量化及存储</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/13EmbeddingText.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>文本向量化分类</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/14DynamicFunctionCall.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>动态函数调用</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0 has-active" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>RAG</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/15RAGAPI.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>RAG API 基础</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/15RAGAPI2.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>RAG API 增强</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/16EasyRag.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>RAG Easy 快速上手</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/17RAGRank.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>RAG 结果重排</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/18WebSearch.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>函数增强搜索</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/19SensitiveWord.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型敏感词处理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/course/docs/20Rag.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>RAG 进阶分享</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-ed470ed6 data-v-ee4370c0><div class="VPDoc has-sidebar has-aside" data-v-ee4370c0 data-v-479a6568><!--[--><!--]--><div class="container" data-v-479a6568><div class="aside" data-v-479a6568><div class="aside-curtain" data-v-479a6568></div><div class="aside-container" data-v-479a6568><div class="aside-content" data-v-479a6568><div class="VPDocAside" data-v-479a6568 data-v-312359a6><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-312359a6 data-v-f9862b92><div class="content" data-v-f9862b92><div class="outline-marker" data-v-f9862b92></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f9862b92>On this page</div><ul class="VPDocOutlineItem root" data-v-f9862b92 data-v-316b2cab><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-312359a6></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-479a6568><div class="content-container" data-v-479a6568><!--[--><!--]--><main class="main" data-v-479a6568><div style="position:relative;" class="vp-doc _course_docs_20Rag" data-v-479a6568><div><h1 id="基于上下文的检索-增强-ai-模型知识检索能力" tabindex="-1"><a href="https://www.anthropic.com/news/contextual-retrieval" target="_blank" rel="noreferrer">基于上下文的检索：增强 AI 模型知识检索能力</a> <a class="header-anchor" href="#基于上下文的检索-增强-ai-模型知识检索能力" aria-label="Permalink to &quot;[基于上下文的检索：增强 AI 模型知识检索能力](https://www.anthropic.com/news/contextual-retrieval)&quot;">​</a></h1><img src="https://minio.pigx.top/oss/202410/1728399617.png" alt="1728399617"><h1 id="上下文检索-显著提高rag性能的方法" tabindex="-1">上下文检索:显著提高RAG性能的方法 <a class="header-anchor" href="#上下文检索-显著提高rag性能的方法" aria-label="Permalink to &quot;上下文检索:显著提高RAG性能的方法&quot;">​</a></h1><p>对于AI模型在特定上下文中使用,通常需要访问背景知识。例如,客户支持聊天机器人需要了解特定业务的相关知识,而法律分析师机器人则需要了解大量过往案例。</p><p>开发者通常使用检索增强生成(RAG)来增强AI模型的知识。RAG是一种从知识库中检索相关信息并将其附加到用户提示中的方法,显著增强了模型的响应。问题在于传统的RAG解决方案在编码信息时会丢失上下文,这通常会导致系统无法从知识库中检索到相关信息。</p><p>在本文中,我们概述了一种显著提高RAG检索步骤性能的方法。该方法称为&quot;上下文检索&quot;,并使用两种子技术:上下文向量(Contextual Embeddings)和上下文BM25。这种方法可以将检索失败率降低49%,如果再结合重排序技术,甚至可以降低67%。这些表示显著的性能提升,直接提高了下游任务的性能。</p><p>你可以轻松地使用Claude部署自己的上下文检索解决方案,只需参考我们的食谱。</p><h2 id="使用更长的提示的注意事项" tabindex="-1">使用更长的提示的注意事项 <a class="header-anchor" href="#使用更长的提示的注意事项" aria-label="Permalink to &quot;使用更长的提示的注意事项&quot;">​</a></h2><p>有时候最简单的方法是最好的。如果你的知识库小于200,000个标记(大约500页的材料),你可以将整个知识库包含在提供给模型的提示中,而不需要RAG或类似的方法。</p><p>几周前,我们为Claude发布了提示缓存,使这种方法显著更快和更节省成本。开发者现在可以在API调用之间缓存经常使用的提示,将延迟减少超过2倍,并将成本降低多达90%(你可以通过阅读我们的提示缓存食谱来了解它是如何工作的)。</p><p>然而,随着你的知识库的增长,你需要一个更可扩展的解决方案。这就是上下文检索的用武之地。</p><h2 id="rag-扩展到更大的知识库" tabindex="-1">RAG:扩展到更大的知识库 <a class="header-anchor" href="#rag-扩展到更大的知识库" aria-label="Permalink to &quot;RAG:扩展到更大的知识库&quot;">​</a></h2><p>对于那些不适合上下文窗口的知识库,RAG是典型的解决方案。RAG通过以下步骤预处理知识库:</p><ol><li>将知识库(文档的&quot;语料库&quot;)分解为较小的文本块,通常不超过几百个标记;</li><li>使用向量模型将这些块转换为向量向量,这些向量编码意义;</li><li>将这些向量存储在允许通过语义相似性搜索的向量数据库中。</li></ol><p>在运行时,当用户向模型输入查询时,使用向量数据库根据语义相似性找到最相关的块。然后,将最相关的块添加到发送给生成模型的提示中。</p><p>虽然向量模型在捕捉语义关系方面表现出色,但它们可能会错过关键的精确匹配。幸运的是,有一种古老的技术可以在这方面提供帮助。BM25(最佳匹配25)是一种使用词汇匹配的排名函数,用于找到精确的单词或短语匹配。它特别适用于包含唯一标识符或技术术语的查询。</p><p>BM25 通过利用 TF-IDF(词频-逆文档频率)概念来工作。TF-IDF 衡量一个词在一个集合中的文档中的重要性。BM25 通过考虑文档长度并应用饱和函数来细化这一点,该函数有助于防止常见词在结果中占据主导地位。</p><p>BM25 可以在语义向量失败的地方成功:假设用户在一个技术支持数据库中查询&quot;错误代码 TS-999&quot;。一个向量模型可能会找到关于错误代码的一般内容,但可能会错过确切的&quot;TS-999&quot;匹配。BM25 寻找这个特定的文本字符串来识别相关的文档。</p><p>RAG 解决方案可以通过使用以下步骤结合向量和 BM25 技术来更准确地检索最相关的块:</p><ol><li>将知识库(文档的&quot;语料库&quot;)分解为较小的文本块,通常不超过几百个标记;</li><li>为这些块创建 TF-IDF 编码和语义向量;</li><li>使用 BM25 根据确切匹配找到顶部块;</li><li>使用向量根据语义相似性找到顶部块;</li><li>使用 rank fusion 技术结合和去重 (3) 和 (4) 的结果;</li><li>将顶部块添加到提示中以生成响应。</li></ol><p>通过结合 BM25 和向量模型,传统的 RAG 系统可以提供更全面和准确的结果,平衡精确的词匹配与更广泛的语义理解。</p><img src="https://minio.pigx.top/oss/202410/1728481113.png" alt="1728481113"><p>这种方法允许你有效地扩展到巨大的知识库,远远超过单个提示所能容纳的范围。但这些传统的 RAG 系统有一个显著的限制:它们经常破坏上下文。</p><h2 id="传统rag中的上下文困境" tabindex="-1">传统RAG中的上下文困境 <a class="header-anchor" href="#传统rag中的上下文困境" aria-label="Permalink to &quot;传统RAG中的上下文困境&quot;">​</a></h2><p>在传统的 RAG 中,文档通常被拆分为较小的块以进行高效检索。虽然这种方法对许多应用效果很好,但当单个块缺乏足够的上下文时,可能会导致问题。</p><p>例如,想象一下,你的知识库中包含了一个财务信息集合(例如,美国 SEC 文件),并且你收到了以下问题:&quot;ACME 公司在 2023 年第二季度的收入增长是多少?&quot;</p><p>一个相关的块可能包含以下文本:&quot;公司收入比上一季度增长了 3%。&quot; 然而,这个块本身没有指明它指的是哪家公司或相关时间范围,这使得很难检索到正确的信息或有效地使用信息。</p><h2 id="引入上下文检索" tabindex="-1">引入上下文检索 <a class="header-anchor" href="#引入上下文检索" aria-label="Permalink to &quot;引入上下文检索&quot;">​</a></h2><p>上下文检索通过在每个块之前添加特定于块的解释性上下文,在向量(&quot;上下文向量&quot;)和创建 BM25 索引(&quot;上下文 BM25&quot;)之前解决了这个问题。</p><p>让我们回到我们的 SEC 文件集合示例。以下是一个块可能被转换的方式:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>original_chunk = &quot;公司收入比上一季度增长了 3%。&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>contextualized_chunk = &quot;该段落摘自ACME公司2023年第二季度的SEC文件；上一季度的收入为3.14亿美元。该公司的收入比上一季度增长了3%。&quot;</span></span></code></pre></div><p>值得注意的是,其他方法已经提出使用上下文来提高检索性能。其他建议包括:在块中添加通用文档摘要(我们尝试过并发现效果有限),假设文档向量,以及基于摘要的索引(我们评估过并发现性能较低)。这些方法与本文提出的方法不同。</p><h2 id="实现上下文检索" tabindex="-1">实现上下文检索 <a class="header-anchor" href="#实现上下文检索" aria-label="Permalink to &quot;实现上下文检索&quot;">​</a></h2><p>当然,手动注释知识库中的数千甚至数百万个块将是一项极其繁重的工作。为了实现上下文检索,我们求助于Claude。我们已经编写了一个提示,指示模型提供简洁的、块特定的上下文,使用整个文档的上下文解释块。我们使用以下Claude 3 Haiku提示为每个块生成上下文:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;document&gt; </span></span>
<span class="line"><span>{{WHOLE_DOCUMENT}} </span></span>
<span class="line"><span>&lt;/document&gt; </span></span>
<span class="line"><span>Here is the chunk we want to situate within the whole document </span></span>
<span class="line"><span>&lt;chunk&gt; </span></span>
<span class="line"><span>{{CHUNK_CONTENT}} </span></span>
<span class="line"><span>&lt;/chunk&gt; </span></span>
<span class="line"><span>Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.</span></span></code></pre></div><p>生成的上下文文本,通常为50-100个标记,在向量之前和创建BM25索引之前附加到块。</p><p>以下是预处理流程的实际应用:</p><img src="https://minio.pigx.top/oss/202410/1728481126.png" alt="1728481126"><h2 id="使用提示缓存减少上下文检索的成本" tabindex="-1">使用提示缓存减少上下文检索的成本 <a class="header-anchor" href="#使用提示缓存减少上下文检索的成本" aria-label="Permalink to &quot;使用提示缓存减少上下文检索的成本&quot;">​</a></h2><p>上下文检索在Claude中以低成本实现,得益于上述的特殊提示缓存功能。使用提示缓存,你不需要为每个块传递参考文档。你只需将文档加载到缓存中一次,然后引用之前缓存的内容。假设每个块有800个标记,8k标记的文档,50个标记的上下文指令,每个块有100个标记的上下文,生成上下文块的一次性成本为每百万文档标记1.02美元。</p><h2 id="方法论" tabindex="-1">方法论 <a class="header-anchor" href="#方法论" aria-label="Permalink to &quot;方法论&quot;">​</a></h2><p>我们尝试了各种知识领域(代码库、小说、ArXiv论文、科学论文)、向量模型、检索策略和评估指标。我们在附录II中包含了一些我们用于每个领域的示例问题和答案。</p><p>下图显示了所有知识领域中所有向量配置的平均性能,使用最佳向量配置(Gemini Text 004)检索前20个块。我们使用1减去召回@20作为评估指标,该指标测量在20个块中未检索到的相关文档的百分比。你可以在附录中查看完整的结果——上下文化在所有评估的向量源组合中都提高了性能。</p><h2 id="性能提升" tabindex="-1">性能提升 <a class="header-anchor" href="#性能提升" aria-label="Permalink to &quot;性能提升&quot;">​</a></h2><p>我们的实验表明:</p><ul><li>上下文向量将前20个块的检索失败率降低了35% (5.7% → 3.7%).</li><li>结合上下文向量和上下文BM25将前20个块的检索失败率降低了49% (5.7% → 2.9%).</li></ul><img src="https://minio.pigx.top/oss/202410/1728481137.png" alt="1728481137"><h2 id="实现考虑" tabindex="-1">实现考虑 <a class="header-anchor" href="#实现考虑" aria-label="Permalink to &quot;实现考虑&quot;">​</a></h2><p>在实现上下文检索时,有几点需要考虑:</p><ul><li>Chunk boundaries: 考虑如何将文档拆分为块。块大小、块边界和块重叠的选择会影响检索性能。</li><li>Embedding model: 上下文检索在所有测试的向量模型中都提高了性能,但某些模型可能比其他模型表现更好。我们发现 Gemini 和 Voyage 向量特别有效。</li><li>Custom contextualizer prompts: 虽然我们提供的通用提示效果很好,但你可以使用针对特定领域或用例的提示来实现更好的结果(例如,包括一个可能只在知识库中的其他文档中定义的关键术语词汇表)。</li><li>Number of chunks: 将更多块添加到上下文窗口中会增加包含相关信息的机会。然而,更多的信息可能会分散模型的注意力,因此有一个限制。我们尝试了5、10和20个块,发现使用20个块是这些选项中性能最好的(见附录中的比较),但值得在特定用例上进行实验。</li><li>Always run evals: 响应生成可以通过传递上下文块并区分上下文和块来改进。</li></ul><h2 id="进一步提高性能的重排序" tabindex="-1">进一步提高性能的重排序 <a class="header-anchor" href="#进一步提高性能的重排序" aria-label="Permalink to &quot;进一步提高性能的重排序&quot;">​</a></h2><p>在最后一步中,我们可以将上下文检索与另一种技术结合使用,以获得更多的性能提升。在传统的RAG中,AI系统搜索其知识库以找到可能相关的信息块。使用大型知识库时,此初始检索通常会返回大量块——有时多达数百个,具有不同的相关性和重要性。</p><p>重排序是一种常用的过滤技术,可以确保只将最相关的块传递给模型。重排序提供更好的响应并减少成本和延迟,因为模型处理的信息更少。关键步骤如下:</p><ol><li>执行初始检索以获取可能相关的顶部块(我们使用前150个);</li><li>将前N个块和用户的查询传递给重排序模型;</li><li>使用重排序模型,根据其与提示的相关性和重要性为每个块打分,然后选择前K个块(我们使用前20个);</li><li>将前K个块传递给模型作为上下文,以生成最终结果。</li></ol><img src="https://minio.pigx.top/oss/202410/1728481149.png" alt="1728481149"><h3 id="性能提升-1" tabindex="-1">性能提升 <a class="header-anchor" href="#性能提升-1" aria-label="Permalink to &quot;性能提升&quot;">​</a></h3><p>市场上有几种重排序模型。我们使用 Cohere 重排序器进行了测试。Voyage 也提供了一个重排序器,但我们没有时间测试它。我们的实验表明,在各种领域中,添加重排序步骤可以进一步优化检索。</p><p>具体来说,我们发现重排序的上下文向量和上下文 BM25 将前 20 个块的检索失败率降低了 67%(5.7% → 1.9%)。</p><img src="https://minio.pigx.top/oss/202410/1728481154.png" alt="1728481154"><h3 id="成本和延迟考虑" tabindex="-1">成本和延迟考虑 <a class="header-anchor" href="#成本和延迟考虑" aria-label="Permalink to &quot;成本和延迟考虑&quot;">​</a></h3><p>重排序的一个重要考虑因素是对延迟和成本的影响，特别是在重排序大量块时。由于重排序在运行时添加了额外的步骤，即使重排序器并行对所有块进行评分，它也不可避免地会增加一些延迟。在重排序更多块以获得更好性能与重排序更少块以降低延迟和成本之间存在固有的权衡。我们建议在您的特定用例上尝试不同的设置，以找到合适的平衡点。</p><h2 id="结论" tabindex="-1">结论 <a class="header-anchor" href="#结论" aria-label="Permalink to &quot;结论&quot;">​</a></h2><p>我们进行了大量测试，比较了上述所有技术的不同组合（向量模型、使用 BM25、使用上下文检索、使用重排序器以及检索的前 K 个结果总数），涵盖了各种不同的数据集类型。以下是我们的发现总结：</p><ol><li>向量+BM25 比单独使用向量更好；</li><li>在我们测试的向量中，Voyage 和 Gemini 的效果最好；</li><li>将前 20 个块传递给模型比仅传递前 10 个或前 5 个更有效；</li><li>为块添加上下文大大提高了检索准确性；</li><li>使用重排序比不使用重排序更好；</li><li><strong>所有这些优势都可以叠加</strong>：为了最大化性能提升，我们可以结合上下文向量（来自 Voyage 或 Gemini）、上下文 BM25、重排序步骤，并将 20 个块添加到提示中。</li></ol><p>我们鼓励所有使用知识库的开发者使用<strong>我们的指南</strong>来尝试这些方法，以解锁新的性能水平。</p><h2 id="附录-i" tabindex="-1">附录 I <a class="header-anchor" href="#附录-i" aria-label="Permalink to &quot;附录 I&quot;">​</a></h2><p>以下是各数据集、向量提供商、是否使用 BM25 补充向量、是否使用上下文检索以及是否使用重排序的前 20 个检索结果的细分。</p><p>有关前 10 个和前 5 个检索结果的细分以及每个数据集的示例问题和答案，请参见<strong>附录 II</strong>。</p><img src="https://minio.pigx.top/oss/202410/1728481176.png" alt="1728481176"><p><em>各数据集和向量提供商的 1 减去召回率 @ 20 的结果。</em></p></div></div></main><footer class="VPDocFooter" data-v-479a6568 data-v-a9f44413><!--[--><!--]--><div class="edit-info" data-v-a9f44413><!----><div class="last-updated" data-v-a9f44413><p class="VPLastUpdated" data-v-a9f44413 data-v-0500d987>Last updated: <time datetime="2025-02-04T02:50:54.000Z" data-v-0500d987></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-a9f44413><span class="visually-hidden" id="doc-footer-aria-label" data-v-a9f44413>Pager</span><div class="pager" data-v-a9f44413><a class="VPLink link pager-link prev" href="/course/docs/19SensitiveWord.html" data-v-a9f44413><!--[--><span class="desc" data-v-a9f44413>Previous page</span><span class="title" data-v-a9f44413>模型敏感词处理</span><!--]--></a></div><div class="pager" data-v-a9f44413><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"course_docs_00introduce.md\":\"BPb6ZQc5\",\"course_docs_01models.md\":\"CnZDumaZ\",\"course_docs_02langchain4j.md\":\"BqBm7T7Z\",\"course_docs_03chatapi.md\":\"DuvQpB9r\",\"course_docs_04chatsetting.md\":\"Bua8-QFe\",\"course_docs_05chatstream.md\":\"DfJx_VnK\",\"course_docs_06chatvl.md\":\"xJ6_4RDZ\",\"course_docs_07chatmemory.md\":\"DaapT6rW\",\"course_docs_08prompt.md\":\"BWYJ9LSG\",\"course_docs_09jsonoutput.md\":\"Dw2ZprI_\",\"course_docs_10dynamicjson.md\":\"mdHGuCot\",\"course_docs_11functioncall.md\":\"yaQz7bnt\",\"course_docs_12embedding.md\":\"CxnmfyCR\",\"course_docs_13embeddingtext.md\":\"Dpe1mCeO\",\"course_docs_14dynamicfunctioncall.md\":\"CO9_2-c1\",\"course_docs_15ragapi.md\":\"BB9aU49g\",\"course_docs_15ragapi2.md\":\"B9pCRD4I\",\"course_docs_16easyrag.md\":\"z7wbRbsw\",\"course_docs_17ragrank.md\":\"LSxfsfkU\",\"course_docs_18websearch.md\":\"BVOQ1vm3\",\"course_docs_19sensitiveword.md\":\"6SUyGZI6\",\"course_docs_20rag.md\":\"CFLP7nQ4\",\"index.md\":\"D8q6JMZ6\",\"langchat_docs_deepseek-r1-architecture-and-training.md\":\"BTs4EGCL\",\"langchat_docs_deepseek-r1-distilled-models.md\":\"RDBgSCQ-\",\"langchat_docs_deepseek-r1-reasoning-capabilities-analysis.md\":\"BKFhATbz\",\"langchat_docs_deepseek-r1-tuning.md\":\"AT6KsTy6\",\"langchat_docs_distill-deepseek-r1-into-your-model.md\":\"CJGDJYmj\",\"langchat_docs_introduce.md\":\"CXPF4FN9\",\"langchat_docs_langchat-deepseek-r1.md\":\"ytXPH-Uh\",\"langchat_docs_questions.md\":\"CsJ_8bzt\",\"langchat_docs_rag.md\":\"BqSY2rti\",\"langchat_docs_run_environment.md\":\"D2aGHZfA\",\"langchat_docs_run_getting-started.md\":\"BfocB2e9\",\"langchat_docs_run_login.md\":\"IsvveOf8\",\"langchat_docs_server_knowledge.md\":\"B1g4Kg17\",\"langchat_docs_server_models-proxy.md\":\"3QDtmYNA\",\"langchat_docs_server_models.md\":\"BkmsssDO\",\"langchat_docs_show.md\":\"C0pV5WLq\",\"readme.md\":\"DKy5OORU\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"LangChat Docs\",\"description\":\"LangChat Project Document\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"LangChat\",\"link\":\"/\"},{\"text\":\"LangChat文档\",\"link\":\"/langchat/docs/introduce\",\"activeMatch\":\"/langchat/\"},{\"text\":\"Java AI课程\",\"link\":\"/course/docs/00Introduce\",\"activeMatch\":\"/course/\"},{\"text\":\"在线预览\",\"items\":[{\"text\":\"LangChat LLM Ops\",\"link\":\"http://llm.langchat.cn\"},{\"text\":\"LangChat UPMS Ops\",\"link\":\"http://upms.langchat.cn\"}]}],\"sidebar\":{\"/langchat/\":[{\"text\":\"LangChat实战DeepSeek-R1\",\"link\":\"/langchat/docs/langchat-deepseek-r1\"},{\"text\":\"写在前面\",\"items\":[{\"text\":\"LangChat介绍\",\"link\":\"/langchat/docs/introduce\"},{\"text\":\"RAG基础概念\",\"link\":\"/langchat/docs/rag\"},{\"text\":\"DeepSeek-R1微调指南\",\"link\":\"/langchat/docs/deepseek-r1-tuning\"},{\"text\":\"DeepSeek R1架构和训练过程图解\",\"link\":\"/langchat/docs/deepseek-r1-architecture-and-training\"},{\"text\":\"DeepSeek-R1蒸馏模型\",\"link\":\"/langchat/docs/deepseek-r1-distilled-models\"},{\"text\":\"DeepSeek-R1的推理能力分析\",\"link\":\"/langchat/docs/deepseek-r1-reasoning-capabilities-analysis\"},{\"text\":\"蒸馏DeepSeek-R1到自己的模型\",\"link\":\"/langchat/docs/distill-deepseek-r1-into-your-model\"},{\"text\":\"常见问题\",\"link\":\"/langchat/docs/questions\"},{\"text\":\"更多文档\",\"link\":\"/langchat/docs/show\"}]},{\"text\":\"运行LangChat\",\"items\":[{\"text\":\"环境准备\",\"link\":\"/langchat/docs/run/environment\"},{\"text\":\"快速开始\",\"link\":\"/langchat/docs/run/getting-started\"},{\"text\":\"登录\",\"link\":\"/langchat/docs/run/login\"}]},{\"text\":\"LangChat使用\",\"items\":[{\"text\":\"模型配置\",\"link\":\"/langchat/docs/server/models\"},{\"text\":\"模型代理\",\"link\":\"/langchat/docs/server/models-proxy\"},{\"text\":\"知识库\",\"link\":\"/langchat/docs/server/knowledge\"}]}],\"/course\":[{\"text\":\"LangChain4j基础\",\"items\":[{\"text\":\"开始之前\",\"link\":\"/course/docs/00Introduce\"},{\"text\":\"模型选择\",\"link\":\"/course/docs/01Models\"},{\"text\":\"LangChain4j 介绍\",\"link\":\"/course/docs/02LangChain4j\"},{\"text\":\"Chat API 上手\",\"link\":\"/course/docs/03ChatAPI\"},{\"text\":\"API 进阶配置\",\"link\":\"/course/docs/04ChatSetting\"},{\"text\":\"Chat 流式输出\",\"link\":\"/course/docs/05ChatStream\"},{\"text\":\"Chat 视觉理解\",\"link\":\"/course/docs/06ChatVL\"},{\"text\":\"Chat 记忆缓存\",\"link\":\"/course/docs/07ChatMemory\"},{\"text\":\"提示词工程\",\"link\":\"/course/docs/08Prompt\"},{\"text\":\"JSON 结构化输出\",\"link\":\"/course/docs/09JSONOutput\"},{\"text\":\"业务动态 JSON 结构化\",\"link\":\"/course/docs/10DynamicJSON\"},{\"text\":\"向量化及存储\",\"link\":\"/course/docs/12Embedding\"},{\"text\":\"文本向量化分类\",\"link\":\"/course/docs/13EmbeddingText\"},{\"text\":\"动态函数调用\",\"link\":\"/course/docs/14DynamicFunctionCall\"}]},{\"text\":\"RAG\",\"items\":[{\"text\":\"RAG API 基础\",\"link\":\"/course/docs/15RAGAPI\"},{\"text\":\"RAG API 增强\",\"link\":\"/course/docs/15RAGAPI2\"},{\"text\":\"RAG Easy 快速上手\",\"link\":\"/course/docs/16EasyRag\"},{\"text\":\"RAG 结果重排\",\"link\":\"/course/docs/17RAGRank\"},{\"text\":\"函数增强搜索\",\"link\":\"/course/docs/18WebSearch\"},{\"text\":\"模型敏感词处理\",\"link\":\"/course/docs/19SensitiveWord\"},{\"text\":\"RAG 进阶分享\",\"link\":\"/course/docs/20Rag\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/TyCoding/langchat\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>